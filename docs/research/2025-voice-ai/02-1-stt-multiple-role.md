# STT å¤šè§’è‰²å°è©±é€å­—ç¨¿ç ”ç©¶

> æœ€å¾Œæ›´æ–°ï¼š2026-01

## æ¦‚è¿°

æœ¬æ–‡ç ”ç©¶å„å®¶ STT æœå‹™å°æ–¼å¤šè§’è‰²å°è©±é€å­—ç¨¿ (Speaker Diarization) çš„æ”¯æ´æƒ…æ³ï¼Œç›®æ¨™æ˜¯ç”¢ç”Ÿé¡ä¼¼ `A: xxx B: xxx A: xxx` çš„å°è©±æ ¼å¼é€å­—ç¨¿ã€‚

## ä»€éº¼æ˜¯ Speaker Diarizationï¼Ÿ

Speaker Diarizationï¼ˆèªªè©±è€…åˆ†é›¢ï¼‰æ˜¯ä¸€ç¨®è‡ªå‹•è­˜åˆ¥éŸ³é »ä¸­ã€Œèª°åœ¨ä½•æ™‚èªªè©±ã€çš„æŠ€è¡“ã€‚å®ƒå¯ä»¥ï¼š
- åˆ†é›¢ä¸åŒèªªè©±è€…çš„èªéŸ³æ®µè½
- ç‚ºæ¯å€‹èªéŸ³ç‰‡æ®µæ¨™è¨˜èªªè©±è€…æ¨™ç±¤ï¼ˆå¦‚ Speaker Aã€Speaker Bï¼‰
- ç”¢ç”Ÿå¸¶æœ‰èªªè©±è€…æ¨™è­˜çš„çµæ§‹åŒ–é€å­—ç¨¿

## å„æœå‹™å•† Speaker Diarization æ”¯æ´æ¯”è¼ƒ

### åŸç”Ÿæ”¯æ´å°è©±æ ¼å¼çš„æœå‹™

| æœå‹™ | æœ€å¤§èªªè©±è€…æ•¸ | è¼¸å‡ºæ ¼å¼ | å³æ™‚ä¸²æµ | èªªè©±è€…å‘½å |
|------|-------------|---------|---------|-----------|
| Deepgram | ç„¡æ˜ç¢ºé™åˆ¶ | `speaker: 0, 1, 2...` | âœ… | âŒ éœ€è‡ªè¡Œè™•ç† |
| AssemblyAI | 10+ (å¯èª¿) | `Speaker A, B, C...` | âœ… | âœ… Speaker Identification |
| ElevenLabs Scribe | 32 | `speaker_id` | âš ï¸ éå³æ™‚ | âŒ |
| Google Cloud | 6 (é è¨­, å¯èª¿) | `speaker_tag: 1, 2...` | âœ… | âŒ |
| Azure Speech | å¤šäºº | `Guest-1, Guest-2...` | âœ… | âŒ |
| OpenAI GPT-4o-transcribe | æ”¯æ´ | éœ€ known_speaker | âš ï¸ éå³æ™‚ | âœ… å¯æä¾›åƒè€ƒéŸ³è¨Š |

---

## å„æœå‹™å•†è©³ç´°èªªæ˜

### 1. Deepgram

**æ”¯æ´æ–¹å¼**: åŸç”Ÿ Diarization åŠŸèƒ½

**å•Ÿç”¨åƒæ•¸**:
```python
# å•Ÿç”¨ diarization å’Œ utterances
params = {
    "diarize": True,
    "utterances": True,
    "smart_format": True
}
```

**å›æ‡‰æ ¼å¼**:
```json
{
  "utterances": [
    {
      "speaker": 0,
      "start": 0.0,
      "end": 2.5,
      "text": "ä½ å¥½ï¼Œæˆ‘æ˜¯å®¢æœäººå“¡"
    },
    {
      "speaker": 1,
      "start": 2.8,
      "end": 5.0,
      "text": "ä½ å¥½ï¼Œæˆ‘æƒ³è©¢å•è¨‚å–®å•é¡Œ"
    }
  ]
}
```

**è½‰æ›ç‚ºå°è©±æ ¼å¼**:
```python
def format_conversation(utterances):
    speaker_map = {0: "A", 1: "B", 2: "C"}
    result = []
    for u in utterances:
        label = speaker_map.get(u["speaker"], f"Speaker{u['speaker']}")
        result.append(f"{label}: {u['text']}")
    return "\n".join(result)
```

**ç‰¹é»**:
- ğŸŸ¢ å³æ™‚ä¸²æµæ”¯æ´
- ğŸŸ¢ èªªè©±è€…é‡ç–Šåµæ¸¬
- ğŸŸ¡ åªæä¾›æ•¸å­—æ¨™ç±¤ï¼Œéœ€è‡ªè¡Œè½‰æ›

**åƒè€ƒ**: [Deepgram Diarization Docs](https://developers.deepgram.com/docs/diarization)

---

### 2. AssemblyAI

**æ”¯æ´æ–¹å¼**: Speaker Labels + Speaker Identification

**å•Ÿç”¨åƒæ•¸**:
```python
import assemblyai as aai

config = aai.TranscriptionConfig(
    speaker_labels=True,
    speakers_expected=2  # å¯é¸ï¼Œé æœŸèªªè©±è€…æ•¸é‡
)
```

**å›æ‡‰æ ¼å¼**:
```json
{
  "utterances": [
    {
      "speaker": "A",
      "text": "ä½ å¥½ï¼Œè«‹å•æœ‰ä»€éº¼éœ€è¦å¹«å¿™çš„å—ï¼Ÿ",
      "start": 0,
      "end": 3500
    },
    {
      "speaker": "B",
      "text": "æˆ‘æƒ³æŸ¥è©¢æˆ‘çš„è¨‚å–®ç‹€æ…‹",
      "start": 3800,
      "end": 6200
    }
  ]
}
```

**Speaker Identificationï¼ˆé€²éšï¼‰**:
```python
config = aai.TranscriptionConfig(
    speaker_labels=True,
    speaker_identification={
        "speakers": [
            {"name": "å®¢æœ", "role": "agent"},
            {"name": "é¡§å®¢", "role": "customer"}
        ]
    }
)
```

**ç‰¹é»**:
- ğŸŸ¢ åŸç”Ÿ A/B/C æ ¼å¼
- ğŸŸ¢ æ”¯æ´ Speaker Identificationï¼ˆå°‡æ¨™ç±¤è½‰ç‚ºå§“å/è§’è‰²ï¼‰
- ğŸŸ¢ æœ€å¤š 10+ èªªè©±è€…
- ğŸŸ¡ å»ºè­°æ¯ä½èªªè©±è€…è‡³å°‘é€£çºŒèªª 30 ç§’ä»¥æé«˜æº–ç¢ºåº¦

**åƒè€ƒ**: [AssemblyAI Speaker Diarization](https://www.assemblyai.com/docs/pre-recorded-audio/speaker-diarization)

---

### 3. ElevenLabs Scribe

**æ”¯æ´æ–¹å¼**: Speaker Diarization

**API åƒæ•¸**:
```python
from elevenlabs import ElevenLabs

client = ElevenLabs(api_key=api_key)
result = client.speech_to_text.convert(
    audio=audio_file,
    model_id="scribe_v2",
    diarize=True,
    max_speakers=32
)
```

**ç‰¹é»**:
- ğŸŸ¢ æœ€å¤šæ”¯æ´ 32 ä½èªªè©±è€…
- ğŸŸ¢ 5 è²é“å¤šè²é“æ”¯æ´
- ğŸŸ¡ éå³æ™‚è™•ç†ï¼ˆæ‰¹æ¬¡è½‰éŒ„ï¼‰
- ğŸŸ¡ éœ€è‡ªè¡Œå°‡ speaker_id è½‰ç‚ºæ¨™ç±¤

**åƒè€ƒ**: [ElevenLabs Transcription](https://elevenlabs.io/docs/overview/capabilities/speech-to-text)

---

### 4. Google Cloud Speech-to-Text

**æ”¯æ´æ–¹å¼**: Speaker Diarization Config

**å•Ÿç”¨åƒæ•¸**:
```python
from google.cloud import speech

diarization_config = speech.SpeakerDiarizationConfig(
    enable_speaker_diarization=True,
    min_speaker_count=2,
    max_speaker_count=6
)

config = speech.RecognitionConfig(
    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
    language_code="zh-TW",
    diarization_config=diarization_config
)
```

**å›æ‡‰æ ¼å¼**:
```json
{
  "results": [{
    "alternatives": [{
      "words": [
        {"word": "ä½ å¥½", "speakerTag": 1},
        {"word": "è«‹å•", "speakerTag": 2}
      ]
    }]
  }]
}
```

**ç‰¹é»**:
- ğŸŸ¢ æ”¯æ´å³æ™‚ä¸²æµ
- ğŸŸ¡ é è¨­æœ€å¤š 6 äººï¼Œå¯èª¿æ•´
- ğŸŸ¡ å›å‚³ word-level speaker tagï¼Œéœ€è‡ªè¡Œçµ„åˆæˆå¥å­

**åƒè€ƒ**: [Google Cloud Multiple Voices](https://cloud.google.com/speech-to-text/v2/docs/multiple-voices)

---

### 5. Azure Speech Services

**æ”¯æ´æ–¹å¼**: Real-time Diarization

**å•Ÿç”¨åƒæ•¸**:
```python
speech_config = speechsdk.SpeechConfig(subscription=key, region=region)
speech_config.request_word_level_timestamps = True

auto_detect = speechsdk.AutoDetectSourceLanguageConfig(languages=["zh-TW"])
conversation_transcriber = speechsdk.transcription.ConversationTranscriber(
    speech_config=speech_config
)
```

**è¼¸å‡ºæ ¼å¼**:
èªªè©±è€…æ¨™ç±¤ç‚º `Guest-1`, `Guest-2` ç­‰æ ¼å¼

**ç‰¹é»**:
- ğŸŸ¢ å³æ™‚å°è©±è½‰éŒ„
- ğŸŸ¢ ä¼æ¥­ç´šæ”¯æ´
- ğŸŸ¡ èªªè©±è€…æ¨™ç±¤æ ¼å¼å›ºå®š

**åƒè€ƒ**: [Azure Diarization Quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-stt-diarization)

---

### 6. OpenAI GPT-4o-transcribe-diarize

**æ”¯æ´æ–¹å¼**: å¯æä¾›å·²çŸ¥èªªè©±è€…åƒè€ƒéŸ³è¨Š

**ç‰¹æ®ŠåŠŸèƒ½**: å¯ä¸Šå‚³ 2-10 ç§’çš„åƒè€ƒéŸ³è¨Šä¾†è­˜åˆ¥ç‰¹å®šèªªè©±è€…

```python
import openai

response = openai.Audio.transcribe(
    model="gpt-4o-transcribe-diarize",
    file=audio_file,
    known_speaker_names=["Alice", "Bob"],
    known_speaker_references=[alice_audio, bob_audio],
    chunking_strategy="auto"  # éŸ³è¨Š > 30 ç§’æ™‚éœ€è¨­å®š
)
```

**ç‰¹é»**:
- ğŸŸ¢ å¯ç”¨åƒè€ƒéŸ³è¨Šè­˜åˆ¥èªªè©±è€…
- ğŸŸ¡ æœ€é•· 25 åˆ†é˜ï¼ˆ1500 ç§’ï¼‰
- ğŸŸ¡ åƒ…æ”¯æ´æ‰¹æ¬¡è½‰éŒ„ï¼Œä¸æ”¯æ´å³æ™‚ API

**åƒè€ƒ**: [OpenAI Speech to Text](https://platform.openai.com/docs/guides/speech-to-text)

---

## å¤šè²é“ vs èªªè©±è€…åˆ†é›¢

### ä½•æ™‚ä½¿ç”¨å¤šè²é“ (Multichannel)

é©ç”¨æ–¼ï¼š
- æ¯ä½èªªè©±è€…æœ‰ç¨ç«‹éŒ„éŸ³è»Œé“ï¼ˆå¦‚é›»è©±å®¢æœã€Podcast åˆ†è»ŒéŒ„éŸ³ï¼‰
- éœ€è¦ 100% æº–ç¢ºçš„èªªè©±è€…åˆ†é›¢

```python
# Deepgram å¤šè²é“ç¯„ä¾‹
params = {
    "multichannel": True,
    "channels": 2
}
```

### ä½•æ™‚ä½¿ç”¨ Diarization

é©ç”¨æ–¼ï¼š
- å–®ä¸€éŸ³è»ŒéŒ„éŸ³ï¼ˆå¦‚ç¾å ´æœƒè­°ã€è¨ªè«‡ï¼‰
- ç„¡æ³•é å…ˆåˆ†é›¢éŸ³è»Œ

---

## è‡ªè¡Œå¯¦ä½œå°è©±æ ¼å¼ï¼ˆåˆ†æ®µåˆä½µç­–ç•¥ï¼‰

å¦‚æœ STT æœå‹™ä¸æ”¯æ´åŸç”Ÿ Diarizationï¼Œå¯è€ƒæ…®ä»¥ä¸‹ç­–ç•¥ï¼š

### ç­–ç•¥ 1: åˆ†è»ŒéŒ„éŸ³ + åˆä½µ

```
[éŒ„éŸ³éšæ®µ]
â”œâ”€â”€ èªªè©±è€… A â†’ éŸ³è»Œ 1 â†’ è½‰éŒ„ â†’ A çš„é€å­—ç¨¿
â””â”€â”€ èªªè©±è€… B â†’ éŸ³è»Œ 2 â†’ è½‰éŒ„ â†’ B çš„é€å­—ç¨¿

[åˆä½µéšæ®µ]
æ ¹æ“šæ™‚é–“æˆ³äº¤éŒ¯åˆä½µï¼š
A: [0:00-0:05] ä½ å¥½
B: [0:06-0:10] ä½ å¥½ï¼Œè«‹å•...
A: [0:11-0:15] æ²’å•é¡Œ
```

**å¯¦ä½œç¯„ä¾‹**:
```python
def merge_transcripts(transcript_a: list, transcript_b: list) -> str:
    """åˆä½µå…©å€‹å¸¶æ™‚é–“æˆ³çš„é€å­—ç¨¿"""
    all_segments = []

    for seg in transcript_a:
        all_segments.append({
            "speaker": "A",
            "start": seg["start"],
            "end": seg["end"],
            "text": seg["text"]
        })

    for seg in transcript_b:
        all_segments.append({
            "speaker": "B",
            "start": seg["start"],
            "end": seg["end"],
            "text": seg["text"]
        })

    # ä¾æ™‚é–“æ’åº
    all_segments.sort(key=lambda x: x["start"])

    # æ ¼å¼åŒ–è¼¸å‡º
    result = []
    for seg in all_segments:
        result.append(f"{seg['speaker']}: {seg['text']}")

    return "\n".join(result)
```

### ç­–ç•¥ 2: VAD + åˆ†æ®µ + èªªè©±è€…åˆ†é¡

```
[å–®ä¸€éŸ³è»Œ]
    â†“
[VAD åµæ¸¬] â†’ åˆ‡åˆ†æˆèªéŸ³ç‰‡æ®µ
    â†“
[ç‰¹å¾µæå–] â†’ æå–æ¯æ®µçš„è²ç´‹ç‰¹å¾µ
    â†“
[èšé¡åˆ†æ] â†’ å°‡ç›¸ä¼¼è²ç´‹åˆ†åˆ°åŒä¸€èªªè©±è€…
    â†“
[æ¨™ç±¤è¼¸å‡º] â†’ A: xxx B: xxx
```

**é–‹æºå·¥å…·**:
- [pyannote-audio](https://github.com/pyannote/pyannote-audio) - å¼·å¤§çš„èªªè©±è€…åˆ†é›¢æ¨¡å‹
- [speechbrain](https://github.com/speechbrain/speechbrain) - åŒ…å« speaker embedding

---

## æ¨è–¦æ–¹æ¡ˆ

### ä¾å ´æ™¯é¸æ“‡

| å ´æ™¯ | æ¨è–¦æœå‹™ | åŸå›  |
|------|---------|------|
| å³æ™‚å°è©±è½‰éŒ„ | Deepgram / AssemblyAI | ä½å»¶é² + åŸç”Ÿ diarization |
| å·²çŸ¥èªªè©±è€…åç¨± | AssemblyAI (Speaker ID) | å¯ç›´æ¥è¼¸å‡ºå§“å/è§’è‰² |
| å¤§å‹æœƒè­° (10+ äºº) | ElevenLabs Scribe | æœ€å¤š 32 äºº |
| éœ€è¦åƒè€ƒéŸ³è¨Šè­˜åˆ¥ | OpenAI GPT-4o | å¯æä¾›èªªè©±è€…åƒè€ƒ |
| å¤šè²é“éŒ„éŸ³ | Deepgram Multichannel | 100% æº–ç¢ºåˆ†é›¢ |
| è‡ªå»ºæ–¹æ¡ˆ | pyannote + Whisper | å®Œå…¨è‡ªæ§ï¼Œç„¡ API è²»ç”¨ |

### å°è©±æ ¼å¼è¼¸å‡ºç¯„ä¾‹

```
å®¢æœ: æ‚¨å¥½ï¼Œé€™è£¡æ˜¯å®¢æœä¸­å¿ƒï¼Œè«‹å•æœ‰ä»€éº¼å¯ä»¥å¹«æ‚¨çš„ï¼Ÿ
é¡§å®¢: æˆ‘æƒ³æŸ¥è©¢è¨‚å–® 12345 çš„é…é€ç‹€æ…‹ã€‚
å®¢æœ: å¥½çš„ï¼Œè«‹ç¨ç­‰æˆ‘ç‚ºæ‚¨æŸ¥è©¢ã€‚
é¡§å®¢: è¬è¬ã€‚
å®¢æœ: æŸ¥åˆ°äº†ï¼Œæ‚¨çš„è¨‚å–®ç›®å‰æ­£åœ¨é…é€ä¸­ï¼Œé è¨ˆæ˜å¤©é€é”ã€‚
é¡§å®¢: å¤ªå¥½äº†ï¼Œè¬è¬æ‚¨çš„å¹«åŠ©ã€‚
```

---

## æº–ç¢ºåº¦æ³¨æ„äº‹é …

1. **é€£çºŒèªªè©±æ™‚é–“**: æ¯ä½èªªè©±è€…å»ºè­°è‡³å°‘é€£çºŒèªª 30 ç§’ï¼ŒçŸ­å¥ï¼ˆå¦‚ã€Œæ˜¯ã€ã€ã€Œå¥½ã€ï¼‰è¼ƒé›£æº–ç¢ºåˆ†é¡
2. **è²éŸ³ç›¸ä¼¼åº¦**: åŒæ€§åˆ¥æˆ–è²éŸ³ç›¸ä¼¼çš„èªªè©±è€…æº–ç¢ºåº¦è¼ƒä½
3. **äº¤å‰å°è©±**: å¤šäººåŒæ™‚èªªè©±æ™‚æº–ç¢ºåº¦ä¸‹é™
4. **éŸ³è³ªå½±éŸ¿**: èƒŒæ™¯å™ªéŸ³æœƒå½±éŸ¿èªªè©±è€…åˆ†é›¢æ•ˆæœ
5. **å³æ™‚ vs æ‰¹æ¬¡**: æ‰¹æ¬¡è™•ç†å¯ç²å¾—æ›´é«˜æº–ç¢ºåº¦ï¼ˆæœ‰å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰

---

## æ›´æ–°æ—¥èªŒ

| æ—¥æœŸ | è®Šæ›´ |
|------|------|
| 2026-01 | åˆå§‹ç‰ˆæœ¬ |
