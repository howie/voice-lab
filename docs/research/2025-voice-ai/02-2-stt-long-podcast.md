# STT é•·æ™‚é–“éŸ³æª”è™•ç†ç ”ç©¶

> æœ€å¾Œæ›´æ–°ï¼š2026-01

## æ¦‚è¿°

æœ¬æ–‡ç ”ç©¶å„å®¶ STT æœå‹™å°é•·æ™‚é–“éŸ³æª”ï¼ˆå¦‚ Podcastã€æœƒè­°éŒ„éŸ³ï¼‰çš„æ”¯æ´æƒ…æ³ï¼ŒåŒ…æ‹¬åŸç”Ÿæ”¯æ´çš„æ™‚é•·é™åˆ¶ã€åˆ†æ®µè™•ç†ç­–ç•¥ï¼Œä»¥åŠè‡ªè¡Œå¯¦ä½œçš„æœ€ä½³å¯¦è¸ã€‚

## å„æœå‹™å•†æ™‚é•·é™åˆ¶æ¯”è¼ƒ

| æœå‹™ | æœ€å¤§æ™‚é•· | æœ€å¤§æª”æ¡ˆå¤§å° | è™•ç†æ–¹å¼ | å‚™è¨» |
|------|---------|-------------|---------|------|
| **ElevenLabs** | 10 å°æ™‚ | 3 GB | æ‰¹æ¬¡ï¼ˆå…§éƒ¨ä¸¦è¡Œåˆ†æ®µï¼‰ | ğŸ† æœ€é•·æ”¯æ´ |
| **Google Cloud** | 8 å°æ™‚ (480 åˆ†é˜) | ä¾æ“š GCS | æ‰¹æ¬¡ï¼ˆéåŒæ­¥ï¼‰ | éœ€å­˜æ”¾ Cloud Storage |
| **AssemblyAI** | ç„¡æ˜ç¢ºé™åˆ¶ | ç„¡æ˜ç¢ºé™åˆ¶ | æ‰¹æ¬¡/ä¸²æµ | æŒ‰æ™‚é•·è¨ˆè²» |
| **Deepgram** | 10-20 åˆ†é˜è™•ç†æ™‚é–“ | ç„¡æ˜ç¢ºé™åˆ¶ | æ‰¹æ¬¡/ä¸²æµ | Nova: 10 åˆ†é˜, Whisper: 20 åˆ†é˜ |
| **Azure Speech** | ä¾æ“šæ–¹æ¡ˆ | ä¾æ“šæ–¹æ¡ˆ | æ‰¹æ¬¡/ä¸²æµ | ä¼æ¥­æ–¹æ¡ˆè¼ƒå¯¬ |
| **OpenAI Whisper API** | 25 åˆ†é˜ | 25 MB | æ‰¹æ¬¡ | GPT-4o-transcribe |
| **OpenAI Whisper (è‡ªæ¶)** | ç„¡é™åˆ¶ | ç„¡é™åˆ¶ | è‡ªè¡Œæ§åˆ¶ | éœ€è‡ªè¡Œåˆ†æ®µ |

---

## å„æœå‹™å•†è©³ç´°èªªæ˜

### 1. ElevenLabs Scribe (æ¨è–¦)

**æœ€å¤§æ”¯æ´**: 10 å°æ™‚ / 3 GB

**ç‰¹é»**:
- ğŸŸ¢ åŸç”Ÿæ”¯æ´è¶…é•·éŸ³æª”
- ğŸŸ¢ å…§éƒ¨è‡ªå‹•åˆ†æ®µä¸¦è¡Œè™•ç†ï¼ˆ8 åˆ†é˜ä»¥ä¸Šè‡ªå‹•åˆ‡æˆ 4 æ®µï¼‰
- ğŸŸ¢ è™•ç†é€Ÿåº¦ï¼š20-50x å³æ™‚é€Ÿåº¦

**API ä½¿ç”¨**:
```python
from elevenlabs import ElevenLabs

client = ElevenLabs(api_key=api_key)

# ç›´æ¥ä¸Šå‚³é•·éŸ³æª”ï¼Œç„¡éœ€è‡ªè¡Œåˆ†æ®µ
result = client.speech_to_text.convert(
    audio=open("podcast_2hours.mp3", "rb"),
    model_id="scribe_v2",
    language_code="zh"
)
```

**å®šåƒ¹**: $0.40/hour

**åƒè€ƒ**: [ElevenLabs Speech to Text](https://elevenlabs.io/docs/capabilities/speech-to-text)

---

### 2. Google Cloud Speech-to-Text

**æœ€å¤§æ”¯æ´**: 8 å°æ™‚ (æ‰¹æ¬¡éåŒæ­¥æ¨¡å¼)

**é™åˆ¶**:
- åŒæ­¥è«‹æ±‚: 60 ç§’
- ä¸²æµè«‹æ±‚: 5 åˆ†é˜
- éåŒæ­¥è«‹æ±‚: 480 åˆ†é˜ (8 å°æ™‚)
- ä¸²æµè¨Šæ¯: 10 MB

**ä½¿ç”¨æ–¹å¼**:
```python
from google.cloud import speech

client = speech.SpeechClient()

# é•·éŸ³æª”éœ€å­˜æ”¾ Cloud Storage
audio = speech.RecognitionAudio(uri="gs://bucket/long_podcast.wav")

config = speech.RecognitionConfig(
    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
    language_code="zh-TW",
    enable_automatic_punctuation=True
)

# ä½¿ç”¨éåŒæ­¥ API
operation = client.long_running_recognize(config=config, audio=audio)
response = operation.result(timeout=3600)  # ç­‰å¾…æœ€å¤š 1 å°æ™‚
```

**æ³¨æ„**: éåŒæ­¥è«‹æ±‚éœ€å°‡éŸ³æª”ä¸Šå‚³è‡³ Google Cloud Storage

**åƒè€ƒ**: [Google Cloud Batch Recognize](https://cloud.google.com/speech-to-text/v2/docs/batch-recognize)

---

### 3. AssemblyAI

**æœ€å¤§æ”¯æ´**: ç„¡æ˜ç¢ºæ™‚é•·é™åˆ¶

**ç‰¹é»**:
- ğŸŸ¢ æ”¯æ´è¶…é•·éŸ³æª”
- ğŸŸ¢ å¯é€é URL æˆ–ç›´æ¥ä¸Šå‚³
- ğŸŸ¢ è‡ªå‹•è™•ç†ï¼Œç„¡éœ€æ‰‹å‹•åˆ†æ®µ

**API ä½¿ç”¨**:
```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"

transcriber = aai.Transcriber()

# ç›´æ¥è½‰éŒ„é•·éŸ³æª”
transcript = transcriber.transcribe(
    "https://example.com/long_podcast.mp3",
    config=aai.TranscriptionConfig(
        language_code="zh",
        speaker_labels=True
    )
)
```

**å®šåƒ¹**: $0.15/hour (æ‰€æœ‰èªè¨€åŒåƒ¹)

**åƒè€ƒ**: [AssemblyAI Docs](https://www.assemblyai.com/docs)

---

### 4. Deepgram

**æœ€å¤§è™•ç†æ™‚é–“**:
- Nova/Base/Enhanced æ¨¡å‹: 10 åˆ†é˜è™•ç†æ™‚é–“
- Whisper æ¨¡å‹: 20 åˆ†é˜è™•ç†æ™‚é–“

**æ³¨æ„**: é€™æ˜¯ã€Œè™•ç†æ™‚é–“ã€é™åˆ¶ï¼ŒééŸ³æª”æ™‚é•·é™åˆ¶ã€‚è¶…æ™‚æœƒè¿”å› 504 Gateway Timeoutã€‚

**å»ºè­°**: é•·éŸ³æª”éœ€è‡ªè¡Œåˆ†æ®µè™•ç†

**API ä½¿ç”¨**:
```python
from deepgram import DeepgramClient

deepgram = DeepgramClient(api_key)

# å°æ–¼é•·éŸ³æª”ï¼Œå»ºè­°åˆ†æ®µè™•ç†
# æˆ–ä½¿ç”¨ä¸²æµæ–¹å¼é€æ­¥å‚³é€
```

**åƒè€ƒ**: [Deepgram Pre-recorded](https://developers.deepgram.com/docs/pre-recorded-audio)

---

### 5. OpenAI Whisper API

**é™åˆ¶**:
- GPT-4o-transcribe: 25 åˆ†é˜ (1500 ç§’)
- Whisper API: 25 MB æª”æ¡ˆå¤§å°

**è¶…éé™åˆ¶æ™‚**: å¿…é ˆè‡ªè¡Œåˆ†æ®µ

**API ä½¿ç”¨**:
```python
import openai

# çŸ­éŸ³æª”ç›´æ¥ä½¿ç”¨
response = openai.Audio.transcribe(
    model="whisper-1",
    file=open("short_audio.mp3", "rb")
)

# é•·éŸ³æª”éœ€åˆ†æ®µè™•ç† (è¦‹ä¸‹æ–¹ç­–ç•¥)
```

**åƒè€ƒ**: [OpenAI Speech to Text](https://platform.openai.com/docs/guides/speech-to-text)

---

## è‡ªè¡Œåˆ†æ®µè™•ç†ç­–ç•¥

ç•¶ STT æœå‹™æœ‰æ™‚é•·é™åˆ¶æ™‚ï¼Œéœ€è¦è‡ªè¡Œå¯¦ä½œåˆ†æ®µè™•ç†ã€‚

### ç­–ç•¥ 1: å›ºå®šæ™‚é•·åˆ†æ®µ

**å„ªé»**: å¯¦ä½œç°¡å–®
**ç¼ºé»**: å¯èƒ½åœ¨å¥å­ä¸­é–“åˆ‡æ–·

```python
from pydub import AudioSegment

def split_audio_fixed(audio_path: str, chunk_minutes: int = 10):
    """å›ºå®šæ™‚é•·åˆ†æ®µ"""
    audio = AudioSegment.from_file(audio_path)
    chunk_length_ms = chunk_minutes * 60 * 1000

    chunks = []
    for i, start in enumerate(range(0, len(audio), chunk_length_ms)):
        chunk = audio[start:start + chunk_length_ms]
        chunk_path = f"/tmp/chunk_{i}.mp3"
        chunk.export(chunk_path, format="mp3")
        chunks.append({
            "path": chunk_path,
            "start_ms": start,
            "index": i
        })

    return chunks
```

### ç­–ç•¥ 2: éœéŸ³åµæ¸¬åˆ†æ®µ (æ¨è–¦)

**å„ªé»**: åœ¨è‡ªç„¶åœé “è™•åˆ‡åˆ†ï¼Œé¿å…æˆªæ–·å¥å­
**ç¼ºé»**: åˆ†æ®µé•·åº¦ä¸ä¸€è‡´

```python
from pydub import AudioSegment
from pydub.silence import split_on_silence

def split_audio_on_silence(
    audio_path: str,
    min_silence_len: int = 700,      # æœ€å°éœéŸ³é•·åº¦ (ms)
    silence_thresh: int = -40,        # éœéŸ³é–¾å€¼ (dB)
    max_chunk_length: int = 600000    # æœ€å¤§åˆ†æ®µé•·åº¦ 10 åˆ†é˜ (ms)
):
    """åŸºæ–¼éœéŸ³åµæ¸¬åˆ†æ®µ"""
    audio = AudioSegment.from_file(audio_path)

    chunks = split_on_silence(
        audio,
        min_silence_len=min_silence_len,
        silence_thresh=silence_thresh,
        keep_silence=200  # ä¿ç•™éƒ¨åˆ†éœéŸ³ä½œç‚ºç·©è¡
    )

    # åˆä½µéçŸ­çš„åˆ†æ®µï¼Œåˆ†å‰²éé•·çš„åˆ†æ®µ
    processed_chunks = []
    current_chunk = AudioSegment.empty()

    for chunk in chunks:
        if len(current_chunk) + len(chunk) < max_chunk_length:
            current_chunk += chunk
        else:
            if len(current_chunk) > 0:
                processed_chunks.append(current_chunk)
            current_chunk = chunk

    if len(current_chunk) > 0:
        processed_chunks.append(current_chunk)

    return processed_chunks
```

### ç­–ç•¥ 3: VAD (Voice Activity Detection) åˆ†æ®µ

**å„ªé»**: æ›´ç²¾ç¢ºçš„èªéŸ³é‚Šç•Œåµæ¸¬
**ç¼ºé»**: éœ€è¦é¡å¤–ä¾è³´

```python
import webrtcvad
import wave

def split_audio_vad(audio_path: str, aggressiveness: int = 2):
    """ä½¿ç”¨ WebRTC VAD é€²è¡Œåˆ†æ®µ"""
    vad = webrtcvad.Vad(aggressiveness)  # 0-3, 3 æœ€æ¿€é€²

    # è®€å–éŸ³æª” (éœ€è¦ 16-bit PCM)
    with wave.open(audio_path, 'rb') as wf:
        sample_rate = wf.getframerate()
        frames = wf.readframes(wf.getnframes())

    # VAD éœ€è¦ 10/20/30 ms çš„ frame
    frame_duration_ms = 30
    frame_size = int(sample_rate * frame_duration_ms / 1000) * 2

    segments = []
    current_segment_start = 0
    is_speech = False

    for i in range(0, len(frames), frame_size):
        frame = frames[i:i + frame_size]
        if len(frame) < frame_size:
            break

        speech_detected = vad.is_speech(frame, sample_rate)

        if speech_detected and not is_speech:
            current_segment_start = i
            is_speech = True
        elif not speech_detected and is_speech:
            segments.append((current_segment_start, i))
            is_speech = False

    return segments
```

---

## åˆ†æ®µå¾Œåˆä½µè½‰éŒ„çµæœ

### æ™‚é–“æˆ³æ ¡æ­£

åˆ†æ®µå¾Œæ¯å€‹ chunk çš„æ™‚é–“æˆ³æ˜¯ç›¸å°çš„ï¼Œéœ€è¦æ ¡æ­£ç‚ºçµ•å°æ™‚é–“ï¼š

```python
def merge_transcripts_with_timestamps(
    transcripts: list,
    chunk_start_times: list[float]  # æ¯å€‹ chunk çš„èµ·å§‹æ™‚é–“ (ç§’)
) -> dict:
    """åˆä½µå¤šå€‹åˆ†æ®µçš„è½‰éŒ„çµæœï¼Œæ ¡æ­£æ™‚é–“æˆ³"""
    merged_words = []
    merged_text = []

    for i, transcript in enumerate(transcripts):
        offset = chunk_start_times[i]

        # æ ¡æ­£ word-level æ™‚é–“æˆ³
        if "words" in transcript:
            for word in transcript["words"]:
                merged_words.append({
                    "word": word["word"],
                    "start": word["start"] + offset,
                    "end": word["end"] + offset,
                    "confidence": word.get("confidence", 1.0)
                })

        merged_text.append(transcript.get("text", ""))

    return {
        "text": " ".join(merged_text),
        "words": merged_words
    }
```

### è™•ç†é‚Šç•Œå•é¡Œ

åˆ†æ®µé‚Šç•Œå¯èƒ½å°è‡´ï¼š
1. **å¥å­è¢«æˆªæ–·**: ä½¿ç”¨å‰ä¸€æ®µæœ€å¾Œå¹¾ç§’ä½œç‚º context
2. **é‡è¤‡å…§å®¹**: åˆ†æ®µæœ‰é‡ç–Šæ™‚å»é‡
3. **èªªè©±è€…æ¨™ç±¤ä¸ä¸€è‡´**: éœ€è¦è·¨æ®µè½é‡æ–°è­˜åˆ¥

```python
def transcribe_with_overlap(
    audio_chunks: list,
    overlap_seconds: float = 5.0,
    transcribe_fn: callable
) -> str:
    """å¸¶é‡ç–Šçš„åˆ†æ®µè½‰éŒ„"""
    results = []
    prev_context = ""

    for i, chunk in enumerate(audio_chunks):
        # ä½¿ç”¨å‰ä¸€æ®µçš„çµå°¾ä½œç‚º prompt
        transcript = transcribe_fn(
            chunk,
            prompt=prev_context  # æä¾›ä¸Šä¸‹æ–‡
        )

        # å»é™¤é‡ç–Šéƒ¨åˆ†çš„é‡è¤‡æ–‡å­—
        if i > 0:
            transcript = remove_overlap(results[-1], transcript, overlap_seconds)

        results.append(transcript)
        prev_context = transcript[-200:]  # ä¿ç•™æœ€å¾Œ 200 å­—ä½œç‚ºä¸‹ä¸€æ®µ context

    return " ".join(results)
```

---

## ä¸¦è¡Œè™•ç†åŠ é€Ÿ

å°æ–¼è¶…é•·éŸ³æª”ï¼Œå¯ä¸¦è¡Œè™•ç†å¤šå€‹åˆ†æ®µï¼š

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def transcribe_parallel(
    audio_chunks: list,
    transcribe_fn: callable,
    max_workers: int = 4
) -> list:
    """ä¸¦è¡Œè½‰éŒ„å¤šå€‹åˆ†æ®µ"""
    loop = asyncio.get_event_loop()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        tasks = [
            loop.run_in_executor(executor, transcribe_fn, chunk)
            for chunk in audio_chunks
        ]
        results = await asyncio.gather(*tasks)

    return results


# ä½¿ç”¨ç¯„ä¾‹
async def process_long_podcast(audio_path: str):
    # 1. åˆ†æ®µ
    chunks = split_audio_on_silence(audio_path)

    # 2. ä¸¦è¡Œè½‰éŒ„
    transcripts = await transcribe_parallel(chunks, transcribe_fn)

    # 3. åˆä½µçµæœ
    final = merge_transcripts_with_timestamps(transcripts, chunk_times)

    return final
```

---

## æ¨è–¦æ–¹æ¡ˆ

### ä¾éŸ³æª”é•·åº¦é¸æ“‡

| éŸ³æª”é•·åº¦ | æ¨è–¦æ–¹æ¡ˆ | åŸå›  |
|---------|---------|------|
| < 25 åˆ†é˜ | OpenAI Whisper | ç°¡å–®ã€é«˜å“è³ª |
| 25 åˆ†é˜ - 2 å°æ™‚ | AssemblyAI / Deepgram | åŸç”Ÿæ”¯æ´ã€æ€§åƒ¹æ¯”é«˜ |
| 2 - 10 å°æ™‚ | ElevenLabs Scribe | å…§å»ºä¸¦è¡Œã€æœ€é•·æ”¯æ´ |
| > 8 å°æ™‚æˆ–è‡ªæ§éœ€æ±‚ | Google Cloud éåŒæ­¥ | 8 å°æ™‚ä¸Šé™ã€ä¼æ¥­ç´š |
| ç„¡é™åˆ¶ + æˆæœ¬æ•æ„Ÿ | Whisper è‡ªæ¶ + åˆ†æ®µ | å®Œå…¨è‡ªæ§ |

### æˆæœ¬ä¼°ç®— (10 å°æ™‚ Podcast)

| æœå‹™ | ä¼°ç®—æˆæœ¬ | å‚™è¨» |
|------|---------|------|
| ElevenLabs | $4.00 | åŸç”Ÿæ”¯æ´ï¼Œæœ€ç°¡å–® |
| AssemblyAI | $1.50 | æ€§åƒ¹æ¯”æœ€é«˜ |
| Deepgram | ~$2.60 | éœ€ç¢ºèªè™•ç†æ™‚é–“é™åˆ¶ |
| Google Cloud | ~$9.60 | ä¼æ¥­ç´š |
| Whisper (è‡ªæ¶) | é›»è²»+GPU æˆæœ¬ | éœ€è‡ªè¡Œç¶­è­· |

---

## å¯¦ä½œå»ºè­°

### 1. å„ªå…ˆä½¿ç”¨åŸç”Ÿé•·éŸ³æª”æ”¯æ´

å¦‚æœé ç®—å…è¨±ï¼Œç›´æ¥ä½¿ç”¨ ElevenLabs æˆ– AssemblyAI çš„åŸç”Ÿé•·éŸ³æª”æ”¯æ´ï¼Œé¿å…è‡ªè¡Œå¯¦ä½œåˆ†æ®µé‚è¼¯ã€‚

### 2. å¿…é ˆè‡ªè¡Œåˆ†æ®µæ™‚

1. **ä½¿ç”¨éœéŸ³åµæ¸¬**è€Œéå›ºå®šæ™‚é•·åˆ†æ®µ
2. **åŠ å…¥é‡ç–Šå€åŸŸ**è™•ç†é‚Šç•Œå•é¡Œ
3. **ä¿ç•™ä¸Šä¸‹æ–‡**ä½œç‚ºä¸‹ä¸€æ®µçš„ prompt
4. **ä¸¦è¡Œè™•ç†**åŠ é€Ÿæ•´é«”é€Ÿåº¦
5. **æ ¡æ­£æ™‚é–“æˆ³**ç¢ºä¿æœ€çµ‚çµæœæ­£ç¢º

### 3. ç›£æ§èˆ‡éŒ¯èª¤è™•ç†

```python
def transcribe_with_retry(
    chunk: bytes,
    max_retries: int = 3,
    timeout: int = 300
) -> dict:
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„è½‰éŒ„"""
    for attempt in range(max_retries):
        try:
            return transcribe_fn(chunk, timeout=timeout)
        except TimeoutError:
            if attempt == max_retries - 1:
                raise
            # å¯èƒ½æ˜¯ chunk å¤ªå¤§ï¼Œå˜—è©¦å†åˆ†æ®µ
            continue
        except RateLimitError:
            time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
            continue

    raise Exception("Max retries exceeded")
```

---

## æ›´æ–°æ—¥èªŒ

| æ—¥æœŸ | è®Šæ›´ |
|------|------|
| 2026-01 | åˆå§‹ç‰ˆæœ¬ |
